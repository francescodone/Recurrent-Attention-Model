{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c44BvM2xlwtV"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dqvMkXylyro"
      },
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def enforce_reproducibility(seed=42):\n",
        "  # Sets seed manually for both CPU and CUDA\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  # For atomic operations there is currently no simple way to enforce \n",
        "  # determinism, as the order of parallel operations is not known.\n",
        "  # CUDNN\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  \n",
        "  # System based\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "enforce_reproducibility()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jNuivqikih7"
      },
      "source": [
        "# config.py\n",
        "\n",
        "import argparse\n",
        "\n",
        "arg_lists = []\n",
        "parser = argparse.ArgumentParser(description=\"RAM\")\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in (\"true\", \"1\")\n",
        "\n",
        "\n",
        "def add_argument_group(name):\n",
        "    arg = parser.add_argument_group(name)\n",
        "    arg_lists.append(arg)\n",
        "    return arg\n",
        "\n",
        "\n",
        "# glimpse network params\n",
        "glimpse_arg = add_argument_group(\"Glimpse Network Params\")\n",
        "glimpse_arg.add_argument(\n",
        "    \"--patch_size\", type=int, default=8, help=\"size of extracted patch at highest res\"\n",
        ")\n",
        "glimpse_arg.add_argument(\n",
        "    \"--glimpse_scale\", type=int, default=1, help=\"scale of successive patches\"\n",
        ")\n",
        "glimpse_arg.add_argument(\n",
        "    \"--num_patches\", type=int, default=1, help=\"# of downscaled patches per glimpse\"\n",
        ")\n",
        "glimpse_arg.add_argument(\n",
        "    \"--loc_hidden\", type=int, default=128, help=\"hidden size of loc fc\"\n",
        ")\n",
        "glimpse_arg.add_argument(\n",
        "    \"--glimpse_hidden\", type=int, default=128, help=\"hidden size of glimpse fc\"\n",
        ")\n",
        "\n",
        "\n",
        "# core network params\n",
        "core_arg = add_argument_group(\"Core Network Params\")\n",
        "core_arg.add_argument(\n",
        "    \"--num_glimpses\", type=int, default=6, help=\"# of glimpses, i.e. BPTT iterations\"\n",
        ")\n",
        "core_arg.add_argument(\"--hidden_size\", type=int, default=256, help=\"hidden size of rnn\")\n",
        "\n",
        "\n",
        "# reinforce params\n",
        "reinforce_arg = add_argument_group(\"Reinforce Params\")\n",
        "reinforce_arg.add_argument(\n",
        "    \"--std\", type=float, default=0.05, help=\"gaussian policy standard deviation\"\n",
        ")\n",
        "reinforce_arg.add_argument(\n",
        "    \"--M\", type=int, default=1, help=\"Monte Carlo sampling for valid and test sets\"\n",
        ")\n",
        "\n",
        "\n",
        "# data params\n",
        "data_arg = add_argument_group(\"Data Params\")\n",
        "data_arg.add_argument(\n",
        "    \"--valid_size\",\n",
        "    type=float,\n",
        "    default=0.1,\n",
        "    help=\"Proportion of training set used for validation\",\n",
        ")\n",
        "data_arg.add_argument(\n",
        "    \"--batch_size\", type=int, default=128, help=\"# of images in each batch of data\"\n",
        ")\n",
        "data_arg.add_argument(\n",
        "    \"--num_workers\",\n",
        "    type=int,\n",
        "    default=4,\n",
        "    help=\"# of subprocesses to use for data loading\",\n",
        ")\n",
        "data_arg.add_argument(\n",
        "    \"--shuffle\",\n",
        "    type=str2bool,\n",
        "    default=True,\n",
        "    help=\"Whether to shuffle the train and valid indices\",\n",
        ")\n",
        "data_arg.add_argument(\n",
        "    \"--show_sample\",\n",
        "    type=str2bool,\n",
        "    default=False,\n",
        "    help=\"Whether to visualize a sample grid of the data\",\n",
        ")\n",
        "\n",
        "\n",
        "# training params\n",
        "train_arg = add_argument_group(\"Training Params\")\n",
        "train_arg.add_argument(\n",
        "    \"--is_train\", type=str2bool, default=True, help=\"Whether to train or test the model\"\n",
        ")\n",
        "train_arg.add_argument(\n",
        "    \"--momentum\", type=float, default=0.5, help=\"Nesterov momentum value\"\n",
        ")\n",
        "train_arg.add_argument(\n",
        "    \"--epochs\", type=int, default=200, help=\"# of epochs to train for\"\n",
        ")\n",
        "train_arg.add_argument(\n",
        "    \"--init_lr\", type=float, default=3e-4, help=\"Initial learning rate value\"\n",
        ")\n",
        "train_arg.add_argument(\n",
        "    \"--lr_patience\",\n",
        "    type=int,\n",
        "    default=20,\n",
        "    help=\"Number of epochs to wait before reducing lr\",\n",
        ")\n",
        "train_arg.add_argument(\n",
        "    \"--train_patience\",\n",
        "    type=int,\n",
        "    default=50,\n",
        "    help=\"Number of epochs to wait before stopping train\",\n",
        ")\n",
        "\n",
        "\n",
        "# other params\n",
        "misc_arg = add_argument_group(\"Misc.\")\n",
        "misc_arg.add_argument(\n",
        "    \"--use_gpu\", type=str2bool, default=True, help=\"Whether to run on the GPU\"\n",
        ")\n",
        "misc_arg.add_argument(\n",
        "    \"--best\",\n",
        "    type=str2bool,\n",
        "    default=True,\n",
        "    help=\"Load best model or most recent for testing\",\n",
        ")\n",
        "misc_arg.add_argument(\n",
        "    \"--random_seed\", type=int, default=1, help=\"Seed to ensure reproducibility\"\n",
        ")\n",
        "misc_arg.add_argument(\n",
        "    \"--data_dir\", type=str, default=\"/content/drive/My Drive/RAM/data\", help=\"Directory in which data is stored\"\n",
        ")\n",
        "misc_arg.add_argument(\n",
        "    \"--ckpt_dir\",\n",
        "    type=str,\n",
        "    default=\"/content/drive/My Drive/RAM/ckpt\",\n",
        "    help=\"Directory in which to save model checkpoints\",\n",
        ")\n",
        "misc_arg.add_argument(\n",
        "    \"--logs_dir\",\n",
        "    type=str,\n",
        "    default=\"/content/drive/My Drive/RAM/logs/\",\n",
        "    help=\"Directory in which Tensorboard logs wil be stored\",\n",
        ")\n",
        "misc_arg.add_argument(\n",
        "    \"--use_tensorboard\",\n",
        "    type=str2bool,\n",
        "    default=False,\n",
        "    help=\"Whether to use tensorboard for visualization\",\n",
        ")\n",
        "misc_arg.add_argument(\n",
        "    \"--resume\",\n",
        "    type=str2bool,\n",
        "    default=False,\n",
        "    help=\"Whether to resume training from checkpoint\",\n",
        ")\n",
        "misc_arg.add_argument(\n",
        "    \"--print_freq\",\n",
        "    type=int,\n",
        "    default=10,\n",
        "    help=\"How frequently to print training details\",\n",
        ")\n",
        "misc_arg.add_argument(\n",
        "    \"--plot_freq\", type=int, default=1, help=\"How frequently to plot glimpses\"\n",
        ")\n",
        "\n",
        "\n",
        "def get_config():\n",
        "    config, unparsed = parser.parse_known_args()\n",
        "    return config, unparsed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pcWMLQ1mfFV"
      },
      "source": [
        "# utils.py\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def denormalize(T, coords):\n",
        "    return 0.5 * ((coords + 1.0) * T)\n",
        "\n",
        "\n",
        "def bounding_box(x, y, size, color=\"w\"):\n",
        "    x = int(x - (size / 2))\n",
        "    y = int(y - (size / 2))\n",
        "    rect = patches.Rectangle(\n",
        "        (x, y), size, size, linewidth=1, edgecolor=color, fill=False\n",
        "    )\n",
        "    return rect\n",
        "\n",
        "\n",
        "# https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def resize_array(x, size):\n",
        "    # 3D and 4D tensors allowed only\n",
        "    assert x.ndim in [3, 4], \"Only 3D and 4D Tensors allowed!\"\n",
        "\n",
        "    # 4D Tensor\n",
        "    if x.ndim == 4:\n",
        "        res = []\n",
        "        for i in range(x.shape[0]):\n",
        "            img = array2img(x[i])\n",
        "            img = img.resize((size, size))\n",
        "            img = np.asarray(img, dtype=\"float32\")\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "            img /= 255.0\n",
        "            res.append(img)\n",
        "        res = np.concatenate(res)\n",
        "        res = np.expand_dims(res, axis=1)\n",
        "        return res\n",
        "\n",
        "    # 3D Tensor\n",
        "    img = array2img(x)\n",
        "    img = img.resize((size, size))\n",
        "    res = np.asarray(img, dtype=\"float32\")\n",
        "    res = np.expand_dims(res, axis=0)\n",
        "    res /= 255.0\n",
        "    return res\n",
        "\n",
        "\n",
        "def img2array(data_path, desired_size=None, expand=False, view=False):\n",
        "    \"\"\"\n",
        "    Util function for loading RGB image into a numpy array.\n",
        "\n",
        "    Returns array of shape (1, H, W, C).\n",
        "    \"\"\"\n",
        "    img = Image.open(data_path)\n",
        "    img = img.convert(\"RGB\")\n",
        "    if desired_size:\n",
        "        img = img.resize((desired_size[1], desired_size[0]))\n",
        "    if view:\n",
        "        img.show()\n",
        "    x = np.asarray(img, dtype=\"float32\")\n",
        "    if expand:\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "    x /= 255.0\n",
        "    return x\n",
        "\n",
        "\n",
        "def array2img(x):\n",
        "    \"\"\"\n",
        "    Util function for converting anumpy array to a PIL img.\n",
        "\n",
        "    Returns PIL RGB img.\n",
        "    \"\"\"\n",
        "    x = np.asarray(x)\n",
        "    x = x + max(-np.min(x), 0)\n",
        "    x_max = np.max(x)\n",
        "    if x_max != 0:\n",
        "        x /= x_max\n",
        "    x *= 255\n",
        "    return Image.fromarray(x.astype(\"uint8\"), \"RGB\")\n",
        "\n",
        "\n",
        "def plot_images(images, gd_truth):\n",
        "\n",
        "    images = images.squeeze()\n",
        "    assert len(images) == len(gd_truth) == 9\n",
        "\n",
        "    # Create figure with sub-plots.\n",
        "    fig, axes = plt.subplots(3, 3)\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # plot the image\n",
        "        ax.imshow(images[i], cmap=\"Greys_r\")\n",
        "\n",
        "        xlabel = \"{}\".format(gd_truth[i])\n",
        "        ax.set_xlabel(xlabel)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def prepare_dirs(config):\n",
        "    for path in [config.data_dir, config.ckpt_dir, config.logs_dir]:\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "\n",
        "def save_config(config):\n",
        "    model_name = \"ram_{}_{}x{}_{}\".format(\n",
        "        config.num_glimpses, config.patch_size, config.patch_size, config.glimpse_scale\n",
        "    )\n",
        "    filename = model_name + \"_params.json\"\n",
        "    param_path = os.path.join(config.ckpt_dir, filename)\n",
        "\n",
        "    print(\"[*] Model Checkpoint Dir: {}\".format(config.ckpt_dir))\n",
        "    print(\"[*] Param Path: {}\".format(param_path))\n",
        "\n",
        "    with open(param_path, \"w\") as fp:\n",
        "        json.dump(config.__dict__, fp, indent=4, sort_keys=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLAXZORAmO8f"
      },
      "source": [
        "# data_loader.py\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "def get_train_valid_loader(\n",
        "    data_dir,\n",
        "    batch_size,\n",
        "    random_seed,\n",
        "    valid_size=0.1,\n",
        "    shuffle=True,\n",
        "    show_sample=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=False,\n",
        "):\n",
        "    \"\"\"Train and validation data loaders.\n",
        "\n",
        "    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n",
        "\n",
        "    Args:\n",
        "        data_dir: path directory to the dataset.\n",
        "        batch_size: how many samples per batch to load.\n",
        "        random_seed: fix seed for reproducibility.\n",
        "        valid_size: percentage split of the training set used for\n",
        "            the validation set. Should be a float in the range [0, 1].\n",
        "            In the paper, this number is set to 0.1.\n",
        "        shuffle: whether to shuffle the train/validation indices.\n",
        "        show_sample: plot 9x9 sample grid of the dataset.\n",
        "        num_workers: number of subprocesses to use when loading the dataset.\n",
        "        pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n",
        "            True if using GPU.\n",
        "    \"\"\"\n",
        "    error_msg = \"[!] valid_size should be in the range [0, 1].\"\n",
        "    assert (valid_size >= 0) and (valid_size <= 1), error_msg\n",
        "\n",
        "    # define transforms\n",
        "    normalize = transforms.Normalize((0.1307,), (0.3081,))\n",
        "    trans = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "\n",
        "    # load dataset\n",
        "    dataset = datasets.FashionMNIST(data_dir, train=True, download=True, transform=trans)\n",
        "\n",
        "    num_train = len(dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=train_sampler,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=valid_sampler,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    # visualize some images\n",
        "    if show_sample:\n",
        "        sample_loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=9,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory,\n",
        "        )\n",
        "        data_iter = iter(sample_loader)\n",
        "        images, labels = data_iter.next()\n",
        "        X = images.numpy()\n",
        "        X = np.transpose(X, [0, 2, 3, 1])\n",
        "        plot_images(X, labels)\n",
        "\n",
        "    return (train_loader, valid_loader)\n",
        "\n",
        "\n",
        "def get_test_loader(data_dir, batch_size, num_workers=4, pin_memory=False):\n",
        "    \"\"\"Test datalaoder.\n",
        "\n",
        "    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n",
        "\n",
        "    Args:\n",
        "        data_dir: path directory to the dataset.\n",
        "        batch_size: how many samples per batch to load.\n",
        "        num_workers: number of subprocesses to use when loading the dataset.\n",
        "        pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n",
        "            True if using GPU.\n",
        "    \"\"\"\n",
        "    # define transforms\n",
        "    normalize = transforms.Normalize((0.1307,), (0.3081,))\n",
        "    trans = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "\n",
        "    # load dataset\n",
        "    dataset = datasets.FashionMNIST(data_dir, train=False, download=True, transform=trans)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    return data_loader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS6VvjXbmzIY"
      },
      "source": [
        "# modules.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.distributions import Normal\n",
        "\n",
        "\n",
        "class Retina:\n",
        "    \"\"\"A visual retina.\n",
        "\n",
        "    Extracts a foveated glimpse `phi` around location `l`\n",
        "    from an image `x`.\n",
        "\n",
        "    Concretely, encodes the region around `l` at a\n",
        "    high-resolution but uses a progressively lower\n",
        "    resolution for pixels further from `l`, resulting\n",
        "    in a compressed representation of the original\n",
        "    image `x`.\n",
        "\n",
        "    Args:\n",
        "        x: a 4D Tensor of shape (B, H, W, C). The minibatch\n",
        "            of images.\n",
        "        l: a 2D Tensor of shape (B, 2). Contains normalized\n",
        "            coordinates in the range [-1, 1].\n",
        "        g: size of the first square patch.\n",
        "        k: number of patches to extract in the glimpse.\n",
        "        s: scaling factor that controls the size of\n",
        "            successive patches.\n",
        "\n",
        "    Returns:\n",
        "        phi: a 5D tensor of shape (B, k, g, g, C). The\n",
        "            foveated glimpse of the image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, g, k, s):\n",
        "        self.g = g\n",
        "        self.k = k\n",
        "        self.s = s\n",
        "\n",
        "    def foveate(self, x, l):\n",
        "        \"\"\"Extract `k` square patches of size `g`, centered\n",
        "        at location `l`. The initial patch is a square of\n",
        "        size `g`, and each subsequent patch is a square\n",
        "        whose side is `s` times the size of the previous\n",
        "        patch.\n",
        "\n",
        "        The `k` patches are finally resized to (g, g) and\n",
        "        concatenated into a tensor of shape (B, k, g, g, C).\n",
        "        \"\"\"\n",
        "        phi = []\n",
        "        size = self.g\n",
        "\n",
        "        # extract k patches of increasing size\n",
        "        for i in range(self.k):\n",
        "            phi.append(self.extract_patch(x, l, size))\n",
        "            size = int(self.s * size)\n",
        "\n",
        "        # resize the patches to squares of size g\n",
        "        for i in range(1, len(phi)):\n",
        "            k = phi[i].shape[-1] // self.g\n",
        "            phi[i] = F.avg_pool2d(phi[i], k)\n",
        "\n",
        "        # concatenate into a single tensor and flatten\n",
        "        phi = torch.cat(phi, 1)\n",
        "        phi = phi.view(phi.shape[0], -1)\n",
        "\n",
        "        return phi\n",
        "\n",
        "    def extract_patch(self, x, l, size):\n",
        "        \"\"\"Extract a single patch for each image in `x`.\n",
        "\n",
        "        Args:\n",
        "        x: a 4D Tensor of shape (B, H, W, C). The minibatch\n",
        "            of images.\n",
        "        l: a 2D Tensor of shape (B, 2).\n",
        "        size: a scalar defining the size of the extracted patch.\n",
        "\n",
        "        Returns:\n",
        "            patch: a 4D Tensor of shape (B, size, size, C)\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        start = self.denormalize(H, l)\n",
        "        end = start + size\n",
        "\n",
        "        # pad with zeros\n",
        "        x = F.pad(x, (size // 2, size // 2, size // 2, size // 2))\n",
        "\n",
        "        # loop through mini-batch and extract patches\n",
        "        patch = []\n",
        "        for i in range(B):\n",
        "            patch.append(x[i, :, start[i, 1] : end[i, 1], start[i, 0] : end[i, 0]])\n",
        "        return torch.stack(patch)\n",
        "\n",
        "    def denormalize(self, T, coords):\n",
        "        \"\"\"Convert coordinates in the range [-1, 1] to\n",
        "        coordinates in the range [0, T] where `T` is\n",
        "        the size of the image.\n",
        "        \"\"\"\n",
        "        return (0.5 * ((coords + 1.0) * T)).long()\n",
        "\n",
        "    def exceeds(self, from_x, to_x, from_y, to_y, T):\n",
        "        \"\"\"Check whether the extracted patch will exceed\n",
        "        the boundaries of the image of size `T`.\n",
        "        \"\"\"\n",
        "        if (from_x < 0) or (from_y < 0) or (to_x > T) or (to_y > T):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "class GlimpseNetwork(nn.Module):\n",
        "    \"\"\"The glimpse network.\n",
        "\n",
        "    Combines the \"what\" and the \"where\" into a glimpse\n",
        "    feature vector `g_t`.\n",
        "\n",
        "    - \"what\": glimpse extracted from the retina.\n",
        "    - \"where\": location tuple where glimpse was extracted.\n",
        "\n",
        "    Concretely, feeds the output of the retina `phi` to\n",
        "    a fc layer and the glimpse location vector `l_t_prev`\n",
        "    to a fc layer. Finally, these outputs are fed each\n",
        "    through a fc layer and their sum is rectified.\n",
        "\n",
        "    In other words:\n",
        "\n",
        "        `g_t = relu( fc( fc(l) ) + fc( fc(phi) ) )`\n",
        "\n",
        "    Args:\n",
        "        h_g: hidden layer size of the fc layer for `phi`.\n",
        "        h_l: hidden layer size of the fc layer for `l`.\n",
        "        g: size of the square patches in the glimpses extracted\n",
        "        by the retina.\n",
        "        k: number of patches to extract per glimpse.\n",
        "        s: scaling factor that controls the size of successive patches.\n",
        "        c: number of channels in each image.\n",
        "        x: a 4D Tensor of shape (B, H, W, C). The minibatch\n",
        "            of images.\n",
        "        l_t_prev: a 2D tensor of shape (B, 2). Contains the glimpse\n",
        "            coordinates [x, y] for the previous timestep `t-1`.\n",
        "\n",
        "    Returns:\n",
        "        g_t: a 2D tensor of shape (B, hidden_size).\n",
        "            The glimpse representation returned by\n",
        "            the glimpse network for the current\n",
        "            timestep `t`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, h_g, h_l, g, k, s, c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.retina = Retina(g, k, s)\n",
        "\n",
        "        # glimpse layer\n",
        "        D_in = k * g * g * c\n",
        "        self.fc1 = nn.Linear(D_in, h_g)\n",
        "\n",
        "        # location layer\n",
        "        D_in = 2\n",
        "        self.fc2 = nn.Linear(D_in, h_l)\n",
        "\n",
        "        self.fc3 = nn.Linear(h_g, h_g + h_l)\n",
        "        self.fc4 = nn.Linear(h_l, h_g + h_l)\n",
        "\n",
        "    def forward(self, x, l_t_prev):\n",
        "        # generate glimpse phi from image x\n",
        "        phi = self.retina.foveate(x, l_t_prev)\n",
        "\n",
        "        # flatten location vector\n",
        "        l_t_prev = l_t_prev.view(l_t_prev.size(0), -1)\n",
        "\n",
        "        # feed phi and l to respective fc layers\n",
        "        phi_out = F.relu(self.fc1(phi))\n",
        "        l_out = F.relu(self.fc2(l_t_prev))\n",
        "\n",
        "        what = self.fc3(phi_out)\n",
        "        where = self.fc4(l_out)\n",
        "\n",
        "        # feed to fc layer\n",
        "        g_t = F.relu(what + where)\n",
        "\n",
        "        return g_t\n",
        "\n",
        "\n",
        "class CoreNetwork(nn.Module):\n",
        "    \"\"\"The core network.\n",
        "\n",
        "    An RNN that maintains an internal state by integrating\n",
        "    information extracted from the history of past observations.\n",
        "    It encodes the agent's knowledge of the environment through\n",
        "    a state vector `h_t` that gets updated at every time step `t`.\n",
        "\n",
        "    Concretely, it takes the glimpse representation `g_t` as input,\n",
        "    and combines it with its internal state `h_t_prev` at the previous\n",
        "    time step, to produce the new internal state `h_t` at the current\n",
        "    time step.\n",
        "\n",
        "    In other words:\n",
        "\n",
        "        `h_t = relu( fc(h_t_prev) + fc(g_t) )`\n",
        "\n",
        "    Args:\n",
        "        input_size: input size of the rnn.\n",
        "        hidden_size: hidden size of the rnn.\n",
        "        g_t: a 2D tensor of shape (B, hidden_size). The glimpse\n",
        "            representation returned by the glimpse network for the\n",
        "            current timestep `t`.\n",
        "        h_t_prev: a 2D tensor of shape (B, hidden_size). The\n",
        "            hidden state vector for the previous timestep `t-1`.\n",
        "\n",
        "    Returns:\n",
        "        h_t: a 2D tensor of shape (B, hidden_size). The hidden\n",
        "            state vector for the current timestep `t`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(input_size, hidden_size)\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, g_t, h_t_prev):\n",
        "        h1 = self.i2h(g_t)\n",
        "        h2 = self.h2h(h_t_prev)\n",
        "        h_t = F.relu(h1 + h2)\n",
        "        return h_t\n",
        "\n",
        "\n",
        "class ActionNetwork(nn.Module):\n",
        "    \"\"\"The action network.\n",
        "\n",
        "    Uses the internal state `h_t` of the core network to\n",
        "    produce the final output classification.\n",
        "\n",
        "    Concretely, feeds the hidden state `h_t` through a fc\n",
        "    layer followed by a softmax to create a vector of\n",
        "    output probabilities over the possible classes.\n",
        "\n",
        "    Hence, the environment action `a_t` is drawn from a\n",
        "    distribution conditioned on an affine transformation\n",
        "    of the hidden state vector `h_t`, or in other words,\n",
        "    the action network is simply a linear softmax classifier.\n",
        "\n",
        "    Args:\n",
        "        input_size: input size of the fc layer.\n",
        "        output_size: output size of the fc layer.\n",
        "        h_t: the hidden state vector of the core network\n",
        "            for the current time step `t`.\n",
        "\n",
        "    Returns:\n",
        "        a_t: output probability vector over the classes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, h_t):\n",
        "        a_t = F.log_softmax(self.fc(h_t), dim=1)\n",
        "        return a_t\n",
        "\n",
        "\n",
        "class LocationNetwork(nn.Module):\n",
        "    \"\"\"The location network.\n",
        "\n",
        "    Uses the internal state `h_t` of the core network to\n",
        "    produce the location coordinates `l_t` for the next\n",
        "    time step.\n",
        "\n",
        "    Concretely, feeds the hidden state `h_t` through a fc\n",
        "    layer followed by a tanh to clamp the output beween\n",
        "    [-1, 1]. This produces a 2D vector of means used to\n",
        "    parametrize a two-component Gaussian with a fixed\n",
        "    variance from which the location coordinates `l_t`\n",
        "    for the next time step are sampled.\n",
        "\n",
        "    Hence, the location `l_t` is chosen stochastically\n",
        "    from a distribution conditioned on an affine\n",
        "    transformation of the hidden state vector `h_t`.\n",
        "\n",
        "    Args:\n",
        "        input_size: input size of the fc layer.\n",
        "        output_size: output size of the fc layer.\n",
        "        std: standard deviation of the normal distribution.\n",
        "        h_t: the hidden state vector of the core network for\n",
        "            the current time step `t`.\n",
        "\n",
        "    Returns:\n",
        "        mu: a 2D vector of shape (B, 2).\n",
        "        l_t: a 2D vector of shape (B, 2).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size, std):\n",
        "        super().__init__()\n",
        "\n",
        "        self.std = std\n",
        "\n",
        "        hid_size = input_size // 2\n",
        "        self.fc = nn.Linear(input_size, hid_size)\n",
        "        self.fc_lt = nn.Linear(hid_size, output_size)\n",
        "\n",
        "    def forward(self, h_t):\n",
        "        # compute mean\n",
        "        feat = F.relu(self.fc(h_t.detach()))\n",
        "        mu = torch.tanh(self.fc_lt(feat))\n",
        "\n",
        "        # reparametrization trick\n",
        "        l_t = torch.distributions.Normal(mu, self.std).rsample()\n",
        "        l_t = l_t.detach()\n",
        "        log_pi = Normal(mu, self.std).log_prob(l_t)\n",
        "\n",
        "        # we assume both dimensions are independent\n",
        "        # 1. pdf of the joint is the product of the pdfs\n",
        "        # 2. log of the product is the sum of the logs\n",
        "        log_pi = torch.sum(log_pi, dim=1)\n",
        "\n",
        "        # bound between [-1, 1]\n",
        "        l_t = torch.clamp(l_t, -1, 1)\n",
        "\n",
        "        return log_pi, l_t\n",
        "\n",
        "\n",
        "class BaselineNetwork(nn.Module):\n",
        "    \"\"\"The baseline network.\n",
        "\n",
        "    This network regresses the baseline in the\n",
        "    reward function to reduce the variance of\n",
        "    the gradient update.\n",
        "\n",
        "    Args:\n",
        "        input_size: input size of the fc layer.\n",
        "        output_size: output size of the fc layer.\n",
        "        h_t: the hidden state vector of the core network\n",
        "            for the current time step `t`.\n",
        "\n",
        "    Returns:\n",
        "        b_t: a 2D vector of shape (B, 1). The baseline\n",
        "            for the current time step `t`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, h_t):\n",
        "        b_t = self.fc(h_t.detach())\n",
        "        return b_t\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOiOpDcsmnW3"
      },
      "source": [
        "# model.py\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class RecurrentAttention(nn.Module):\n",
        "    \"\"\"A Recurrent Model of Visual Attention (RAM) [1].\n",
        "\n",
        "    RAM is a recurrent neural network that processes\n",
        "    inputs sequentially, attending to different locations\n",
        "    within the image one at a time, and incrementally\n",
        "    combining information from these fixations to build\n",
        "    up a dynamic internal representation of the image.\n",
        "\n",
        "    References:\n",
        "      [1]: Minh et. al., https://arxiv.org/abs/1406.6247\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, g, k, s, c, h_g, h_l, std, hidden_size, num_classes,\n",
        "    ):\n",
        "        \"\"\"Constructor.\n",
        "\n",
        "        Args:\n",
        "          g: size of the square patches in the glimpses extracted by the retina.\n",
        "          k: number of patches to extract per glimpse.\n",
        "          s: scaling factor that controls the size of successive patches.\n",
        "          c: number of channels in each image.\n",
        "          h_g: hidden layer size of the fc layer for `phi`.\n",
        "          h_l: hidden layer size of the fc layer for `l`.\n",
        "          std: standard deviation of the Gaussian policy.\n",
        "          hidden_size: hidden size of the rnn.\n",
        "          num_classes: number of classes in the dataset.\n",
        "          num_glimpses: number of glimpses to take per image,\n",
        "            i.e. number of BPTT steps.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.std = std\n",
        "\n",
        "        self.sensor = GlimpseNetwork(h_g, h_l, g, k, s, c)\n",
        "        self.rnn = CoreNetwork(hidden_size, hidden_size)\n",
        "        self.locator = LocationNetwork(hidden_size, 2, std)\n",
        "        self.classifier = ActionNetwork(hidden_size, num_classes)\n",
        "        self.baseliner = BaselineNetwork(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x, l_t_prev, h_t_prev, last=False):\n",
        "        \"\"\"Run RAM for one timestep on a minibatch of images.\n",
        "\n",
        "        Args:\n",
        "            x: a 4D Tensor of shape (B, H, W, C). The minibatch\n",
        "                of images.\n",
        "            l_t_prev: a 2D tensor of shape (B, 2). The location vector\n",
        "                containing the glimpse coordinates [x, y] for the previous\n",
        "                timestep `t-1`.\n",
        "            h_t_prev: a 2D tensor of shape (B, hidden_size). The hidden\n",
        "                state vector for the previous timestep `t-1`.\n",
        "            last: a bool indicating whether this is the last timestep.\n",
        "                If True, the action network returns an output probability\n",
        "                vector over the classes and the baseline `b_t` for the\n",
        "                current timestep `t`. Else, the core network returns the\n",
        "                hidden state vector for the next timestep `t+1` and the\n",
        "                location vector for the next timestep `t+1`.\n",
        "\n",
        "        Returns:\n",
        "            h_t: a 2D tensor of shape (B, hidden_size). The hidden\n",
        "                state vector for the current timestep `t`.\n",
        "            mu: a 2D tensor of shape (B, 2). The mean that parametrizes\n",
        "                the Gaussian policy.\n",
        "            l_t: a 2D tensor of shape (B, 2). The location vector\n",
        "                containing the glimpse coordinates [x, y] for the\n",
        "                current timestep `t`.\n",
        "            b_t: a vector of length (B,). The baseline for the\n",
        "                current time step `t`.\n",
        "            log_probas: a 2D tensor of shape (B, num_classes). The\n",
        "                output log probability vector over the classes.\n",
        "            log_pi: a vector of length (B,).\n",
        "        \"\"\"\n",
        "        g_t = self.sensor(x, l_t_prev)\n",
        "        h_t = self.rnn(g_t, h_t_prev)\n",
        "\n",
        "        log_pi, l_t = self.locator(h_t)\n",
        "        b_t = self.baseliner(h_t).squeeze()\n",
        "\n",
        "        if last:\n",
        "            log_probas = self.classifier(h_t)\n",
        "            return h_t, l_t, b_t, log_probas, log_pi\n",
        "\n",
        "        return h_t, l_t, b_t, log_pi\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMxrGHW0nTUZ"
      },
      "source": [
        "!pip install tensorboard_logger "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFGHjINFnGMk"
      },
      "source": [
        "# trainer.py\n",
        "\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tensorboard_logger import configure, log_value\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"A Recurrent Attention Model trainer.\n",
        "\n",
        "    All hyperparameters are provided by the user in the\n",
        "    config file.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, data_loader):\n",
        "        \"\"\"\n",
        "        Construct a new Trainer instance.\n",
        "\n",
        "        Args:\n",
        "            config: object containing command line arguments.\n",
        "            data_loader: A data iterator.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "\n",
        "        if config.use_gpu and torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "        # glimpse network params\n",
        "        self.patch_size = config.patch_size\n",
        "        self.glimpse_scale = config.glimpse_scale\n",
        "        self.num_patches = config.num_patches\n",
        "        self.loc_hidden = config.loc_hidden\n",
        "        self.glimpse_hidden = config.glimpse_hidden\n",
        "\n",
        "        # core network params\n",
        "        self.num_glimpses = config.num_glimpses\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "        # reinforce params\n",
        "        self.std = config.std\n",
        "        self.M = config.M\n",
        "\n",
        "        # data params\n",
        "        if config.is_train:\n",
        "            self.train_loader = data_loader[0]\n",
        "            self.valid_loader = data_loader[1]\n",
        "            self.num_train = len(self.train_loader.sampler.indices)\n",
        "            self.num_valid = len(self.valid_loader.sampler.indices)\n",
        "        else:\n",
        "            self.test_loader = data_loader\n",
        "            self.num_test = len(self.test_loader.dataset)\n",
        "        self.num_classes = 10\n",
        "        self.num_channels = 1\n",
        "\n",
        "        # training params\n",
        "        self.epochs = config.epochs\n",
        "        self.start_epoch = 0\n",
        "        self.momentum = config.momentum\n",
        "        self.lr = config.init_lr\n",
        "\n",
        "        # misc params\n",
        "        self.best = config.best\n",
        "        self.ckpt_dir = config.ckpt_dir\n",
        "        self.logs_dir = config.logs_dir\n",
        "        self.best_valid_acc = 0.0\n",
        "        self.counter = 0\n",
        "        self.lr_patience = config.lr_patience\n",
        "        self.train_patience = config.train_patience\n",
        "        self.use_tensorboard = config.use_tensorboard\n",
        "        self.resume = config.resume\n",
        "        self.print_freq = config.print_freq\n",
        "        self.plot_freq = config.plot_freq\n",
        "        self.model_name = \"ram_{}_{}x{}_{}\".format(\n",
        "            config.num_glimpses,\n",
        "            config.patch_size,\n",
        "            config.patch_size,\n",
        "            config.glimpse_scale,\n",
        "        )\n",
        "\n",
        "        self.plot_dir = \"./plots/\" + self.model_name + \"/\"\n",
        "        if not os.path.exists(self.plot_dir):\n",
        "            os.makedirs(self.plot_dir)\n",
        "\n",
        "        # configure tensorboard logging\n",
        "        if self.use_tensorboard:\n",
        "            tensorboard_dir = self.logs_dir + self.model_name\n",
        "            print(\"[*] Saving tensorboard logs to {}\".format(tensorboard_dir))\n",
        "            if not os.path.exists(tensorboard_dir):\n",
        "                os.makedirs(tensorboard_dir)\n",
        "            configure(tensorboard_dir)\n",
        "\n",
        "        # build RAM model\n",
        "        self.model = RecurrentAttention(\n",
        "            self.patch_size,\n",
        "            self.num_patches,\n",
        "            self.glimpse_scale,\n",
        "            self.num_channels,\n",
        "            self.loc_hidden,\n",
        "            self.glimpse_hidden,\n",
        "            self.std,\n",
        "            self.hidden_size,\n",
        "            self.num_classes,\n",
        "        )\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # initialize optimizer and scheduler\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(), lr=self.config.init_lr\n",
        "        )\n",
        "        self.scheduler = ReduceLROnPlateau(\n",
        "            self.optimizer, \"min\", patience=self.lr_patience\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        h_t = torch.zeros(\n",
        "            self.batch_size,\n",
        "            self.hidden_size,\n",
        "            dtype=torch.float,\n",
        "            device=self.device,\n",
        "            requires_grad=True,\n",
        "        )\n",
        "        l_t = torch.FloatTensor(self.batch_size, 2).uniform_(-1, 1).to(self.device)\n",
        "        l_t.requires_grad = True\n",
        "\n",
        "        return h_t, l_t\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the model on the training set.\n",
        "\n",
        "        A checkpoint of the model is saved after each epoch\n",
        "        and if the validation accuracy is improved upon,\n",
        "        a separate ckpt is created for use on the test set.\n",
        "        \"\"\"\n",
        "        # load the most recent checkpoint\n",
        "        if self.resume:\n",
        "            self.load_checkpoint(best=False)\n",
        "\n",
        "        print(\n",
        "            \"\\n[*] Train on {} samples, validate on {} samples\".format(\n",
        "                self.num_train, self.num_valid\n",
        "            )\n",
        "        )\n",
        "\n",
        "        for epoch in range(self.start_epoch, self.epochs):\n",
        "\n",
        "            print(\n",
        "                \"\\nEpoch: {}/{} - LR: {:.6f}\".format(\n",
        "                    epoch + 1, self.epochs, self.optimizer.param_groups[0][\"lr\"]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # train for 1 epoch\n",
        "            train_loss, train_acc = self.train_one_epoch(epoch)\n",
        "\n",
        "            # evaluate on validation set\n",
        "            valid_loss, valid_acc = self.validate(epoch)\n",
        "\n",
        "            # # reduce lr if validation loss plateaus\n",
        "            self.scheduler.step(-valid_acc)\n",
        "\n",
        "            is_best = valid_acc > self.best_valid_acc\n",
        "            msg1 = \"train loss: {:.3f} - train acc: {:.3f} \"\n",
        "            msg2 = \"- val loss: {:.3f} - val acc: {:.3f} - val err: {:.3f}\"\n",
        "            if is_best:\n",
        "                self.counter = 0\n",
        "                msg2 += \" [*]\"\n",
        "            msg = msg1 + msg2\n",
        "            print(\n",
        "                msg.format(\n",
        "                    train_loss, train_acc, valid_loss, valid_acc, 100 - valid_acc\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # check for improvement\n",
        "            if not is_best:\n",
        "                self.counter += 1\n",
        "            if self.counter > self.train_patience:\n",
        "                print(\"[!] No improvement in a while, stopping training.\")\n",
        "                return\n",
        "            self.best_valid_acc = max(valid_acc, self.best_valid_acc)\n",
        "            self.save_checkpoint(\n",
        "                {\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"model_state\": self.model.state_dict(),\n",
        "                    \"optim_state\": self.optimizer.state_dict(),\n",
        "                    \"best_valid_acc\": self.best_valid_acc,\n",
        "                },\n",
        "                is_best,\n",
        "            )\n",
        "\n",
        "    def train_one_epoch(self, epoch):\n",
        "        \"\"\"\n",
        "        Train the model for 1 epoch of the training set.\n",
        "\n",
        "        An epoch corresponds to one full pass through the entire\n",
        "        training set in successive mini-batches.\n",
        "\n",
        "        This is used by train() and should not be called manually.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        batch_time = AverageMeter()\n",
        "        losses = AverageMeter()\n",
        "        accs = AverageMeter()\n",
        "\n",
        "        tic = time.time()\n",
        "        with tqdm(total=self.num_train) as pbar:\n",
        "            for i, (x, y) in enumerate(self.train_loader):\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                x, y = x.to(self.device), y.to(self.device)\n",
        "\n",
        "                plot = False\n",
        "                if (epoch % self.plot_freq == 0) and (i == 0):\n",
        "                    plot = True\n",
        "\n",
        "                # initialize location vector and hidden state\n",
        "                self.batch_size = x.shape[0]\n",
        "                h_t, l_t = self.reset()\n",
        "\n",
        "                # save images\n",
        "                imgs = []\n",
        "                imgs.append(x[0:9])\n",
        "\n",
        "                # extract the glimpses\n",
        "                locs = []\n",
        "                log_pi = []\n",
        "                baselines = []\n",
        "                for t in range(self.num_glimpses - 1):\n",
        "                    # forward pass through model\n",
        "                    h_t, l_t, b_t, p = self.model(x, l_t, h_t)\n",
        "\n",
        "                    # store\n",
        "                    locs.append(l_t[0:9])\n",
        "                    baselines.append(b_t)\n",
        "                    log_pi.append(p)\n",
        "\n",
        "                # last iteration\n",
        "                h_t, l_t, b_t, log_probas, p = self.model(x, l_t, h_t, last=True)\n",
        "                log_pi.append(p)\n",
        "                baselines.append(b_t)\n",
        "                locs.append(l_t[0:9])\n",
        "\n",
        "                # convert list to tensors and reshape\n",
        "                baselines = torch.stack(baselines).transpose(1, 0)\n",
        "                log_pi = torch.stack(log_pi).transpose(1, 0)\n",
        "\n",
        "                # calculate reward\n",
        "                predicted = torch.max(log_probas, 1)[1]\n",
        "                R = (predicted.detach() == y).float()\n",
        "                R = R.unsqueeze(1).repeat(1, self.num_glimpses)\n",
        "\n",
        "                # compute losses for differentiable modules\n",
        "                loss_action = F.nll_loss(log_probas, y)\n",
        "                loss_baseline = F.mse_loss(baselines, R)\n",
        "\n",
        "                # compute reinforce loss\n",
        "                # summed over timesteps and averaged across batch\n",
        "                adjusted_reward = R - baselines.detach()\n",
        "                loss_reinforce = torch.sum(-log_pi * adjusted_reward, dim=1)\n",
        "                loss_reinforce = torch.mean(loss_reinforce, dim=0)\n",
        "\n",
        "                # sum up into a hybrid loss\n",
        "                loss = loss_action + loss_baseline + loss_reinforce * 0.01\n",
        "\n",
        "                # compute accuracy\n",
        "                correct = (predicted == y).float()\n",
        "                acc = 100 * (correct.sum() / len(y))\n",
        "\n",
        "                # store\n",
        "                losses.update(loss.item(), x.size()[0])\n",
        "                accs.update(acc.item(), x.size()[0])\n",
        "\n",
        "                # compute gradients and update SGD\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # measure elapsed time\n",
        "                toc = time.time()\n",
        "                batch_time.update(toc - tic)\n",
        "\n",
        "                pbar.set_description(\n",
        "                    (\n",
        "                        \"{:.1f}s - loss: {:.3f} - acc: {:.3f}\".format(\n",
        "                            (toc - tic), loss.item(), acc.item()\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "                pbar.update(self.batch_size)\n",
        "\n",
        "                # dump the glimpses and locs\n",
        "                if plot:\n",
        "                    imgs = [g.cpu().data.numpy().squeeze() for g in imgs]\n",
        "                    locs = [l.cpu().data.numpy() for l in locs]\n",
        "                    pickle.dump(\n",
        "                        imgs, open(self.plot_dir + \"g_{}.p\".format(epoch + 1), \"wb\")\n",
        "                    )\n",
        "                    pickle.dump(\n",
        "                        locs, open(self.plot_dir + \"l_{}.p\".format(epoch + 1), \"wb\")\n",
        "                    )\n",
        "\n",
        "                # log to tensorboard\n",
        "                if self.use_tensorboard:\n",
        "                    iteration = epoch * len(self.train_loader) + i\n",
        "                    log_value(\"train_loss\", losses.avg, iteration)\n",
        "                    log_value(\"train_acc\", accs.avg, iteration)\n",
        "\n",
        "            return losses.avg, accs.avg\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, epoch):\n",
        "        \"\"\"Evaluate the RAM model on the validation set.\n",
        "        \"\"\"\n",
        "        losses = AverageMeter()\n",
        "        accs = AverageMeter()\n",
        "\n",
        "        for i, (x, y) in enumerate(self.valid_loader):\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "\n",
        "            # duplicate M times\n",
        "            x = x.repeat(self.M, 1, 1, 1)\n",
        "\n",
        "            # initialize location vector and hidden state\n",
        "            self.batch_size = x.shape[0]\n",
        "            h_t, l_t = self.reset()\n",
        "\n",
        "            # extract the glimpses\n",
        "            log_pi = []\n",
        "            baselines = []\n",
        "            for t in range(self.num_glimpses - 1):\n",
        "                # forward pass through model\n",
        "                h_t, l_t, b_t, p = self.model(x, l_t, h_t)\n",
        "\n",
        "                # store\n",
        "                baselines.append(b_t)\n",
        "                log_pi.append(p)\n",
        "\n",
        "            # last iteration\n",
        "            h_t, l_t, b_t, log_probas, p = self.model(x, l_t, h_t, last=True)\n",
        "            log_pi.append(p)\n",
        "            baselines.append(b_t)\n",
        "\n",
        "            # convert list to tensors and reshape\n",
        "            baselines = torch.stack(baselines).transpose(1, 0)\n",
        "            log_pi = torch.stack(log_pi).transpose(1, 0)\n",
        "\n",
        "            # average\n",
        "            log_probas = log_probas.view(self.M, -1, log_probas.shape[-1])\n",
        "            log_probas = torch.mean(log_probas, dim=0)\n",
        "\n",
        "            baselines = baselines.contiguous().view(self.M, -1, baselines.shape[-1])\n",
        "            baselines = torch.mean(baselines, dim=0)\n",
        "\n",
        "            log_pi = log_pi.contiguous().view(self.M, -1, log_pi.shape[-1])\n",
        "            log_pi = torch.mean(log_pi, dim=0)\n",
        "\n",
        "            # calculate reward\n",
        "            predicted = torch.max(log_probas, 1)[1]\n",
        "            R = (predicted.detach() == y).float()\n",
        "            R = R.unsqueeze(1).repeat(1, self.num_glimpses)\n",
        "\n",
        "            # compute losses for differentiable modules\n",
        "            loss_action = F.nll_loss(log_probas, y)\n",
        "            loss_baseline = F.mse_loss(baselines, R)\n",
        "\n",
        "            # compute reinforce loss\n",
        "            adjusted_reward = R - baselines.detach()\n",
        "            loss_reinforce = torch.sum(-log_pi * adjusted_reward, dim=1)\n",
        "            loss_reinforce = torch.mean(loss_reinforce, dim=0)\n",
        "\n",
        "            # sum up into a hybrid loss\n",
        "            loss = loss_action + loss_baseline + loss_reinforce * 0.01\n",
        "\n",
        "            # compute accuracy\n",
        "            correct = (predicted == y).float()\n",
        "            acc = 100 * (correct.sum() / len(y))\n",
        "\n",
        "            # store\n",
        "            losses.update(loss.item(), x.size()[0])\n",
        "            accs.update(acc.item(), x.size()[0])\n",
        "\n",
        "            # log to tensorboard\n",
        "            if self.use_tensorboard:\n",
        "                iteration = epoch * len(self.valid_loader) + i\n",
        "                log_value(\"valid_loss\", losses.avg, iteration)\n",
        "                log_value(\"valid_acc\", accs.avg, iteration)\n",
        "\n",
        "        return losses.avg, accs.avg\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test(self):\n",
        "        \"\"\"Test the RAM model.\n",
        "\n",
        "        This function should only be called at the very\n",
        "        end once the model has finished training.\n",
        "        \"\"\"\n",
        "        correct = 0\n",
        "\n",
        "        # load the best checkpoint\n",
        "        self.load_checkpoint(best=self.best)\n",
        "\n",
        "        for i, (x, y) in enumerate(self.test_loader):\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "\n",
        "            # duplicate M times\n",
        "            x = x.repeat(self.M, 1, 1, 1)\n",
        "\n",
        "            # initialize location vector and hidden state\n",
        "            self.batch_size = x.shape[0]\n",
        "            h_t, l_t = self.reset()\n",
        "\n",
        "            # extract the glimpses\n",
        "            for t in range(self.num_glimpses - 1):\n",
        "                # forward pass through model\n",
        "                h_t, l_t, b_t, p = self.model(x, l_t, h_t)\n",
        "\n",
        "            # last iteration\n",
        "            h_t, l_t, b_t, log_probas, p = self.model(x, l_t, h_t, last=True)\n",
        "\n",
        "            log_probas = log_probas.view(self.M, -1, log_probas.shape[-1])\n",
        "            log_probas = torch.mean(log_probas, dim=0)\n",
        "\n",
        "            pred = log_probas.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "        perc = (100.0 * correct) / (self.num_test)\n",
        "        error = 100 - perc\n",
        "        print(\n",
        "            \"[*] Test Acc: {}/{} ({:.2f}% - {:.2f}%)\".format(\n",
        "                correct, self.num_test, perc, error\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def save_checkpoint(self, state, is_best):\n",
        "        \"\"\"Saves a checkpoint of the model.\n",
        "\n",
        "        If this model has reached the best validation accuracy thus\n",
        "        far, a seperate file with the suffix `best` is created.\n",
        "        \"\"\"\n",
        "        filename = self.model_name + \"_ckpt.pth.tar\"\n",
        "        ckpt_path = os.path.join(self.ckpt_dir, filename)\n",
        "        torch.save(state, ckpt_path)\n",
        "        if is_best:\n",
        "            filename = self.model_name + \"_model_best.pth.tar\"\n",
        "            shutil.copyfile(ckpt_path, os.path.join(self.ckpt_dir, filename))\n",
        "\n",
        "    def load_checkpoint(self, best=False):\n",
        "        \"\"\"Load the best copy of a model.\n",
        "\n",
        "        This is useful for 2 cases:\n",
        "        - Resuming training with the most recent model checkpoint.\n",
        "        - Loading the best validation model to evaluate on the test data.\n",
        "\n",
        "        Args:\n",
        "            best: if set to True, loads the best model.\n",
        "                Use this if you want to evaluate your model\n",
        "                on the test data. Else, set to False in which\n",
        "                case the most recent version of the checkpoint\n",
        "                is used.\n",
        "        \"\"\"\n",
        "        print(\"[*] Loading model from {}\".format(self.ckpt_dir))\n",
        "\n",
        "        filename = self.model_name + \"_ckpt.pth.tar\"\n",
        "        if best:\n",
        "            filename = self.model_name + \"_model_best.pth.tar\"\n",
        "        ckpt_path = os.path.join(self.ckpt_dir, filename)\n",
        "        ckpt = torch.load(ckpt_path)\n",
        "\n",
        "        # load variables from checkpoint\n",
        "        self.start_epoch = ckpt[\"epoch\"]\n",
        "        self.best_valid_acc = ckpt[\"best_valid_acc\"]\n",
        "        self.model.load_state_dict(ckpt[\"model_state\"])\n",
        "        self.optimizer.load_state_dict(ckpt[\"optim_state\"])\n",
        "\n",
        "        if best:\n",
        "            print(\n",
        "                \"[*] Loaded {} checkpoint @ epoch {} \"\n",
        "                \"with best valid acc of {:.3f}\".format(\n",
        "                    filename, ckpt[\"epoch\"], ckpt[\"best_valid_acc\"]\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            print(\"[*] Loaded {} checkpoint @ epoch {}\".format(filename, ckpt[\"epoch\"]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VJCPXernZo3"
      },
      "source": [
        "# main.py\n",
        "\n",
        "import torch\n",
        "\n",
        "def main(config):\n",
        "    prepare_dirs(config)\n",
        "\n",
        "    # ensure reproducibility\n",
        "    torch.manual_seed(config.random_seed)\n",
        "    kwargs = {}\n",
        "    if config.use_gpu:\n",
        "        torch.cuda.manual_seed(config.random_seed)\n",
        "        kwargs = {\"num_workers\": 1, \"pin_memory\": True}\n",
        "\n",
        "    # instantiate data loaders\n",
        "    if config.is_train:\n",
        "        dloader = get_train_valid_loader(\n",
        "            config.data_dir,\n",
        "            config.batch_size,\n",
        "            config.random_seed,\n",
        "            config.valid_size,\n",
        "            config.shuffle,\n",
        "            config.show_sample,\n",
        "            **kwargs,\n",
        "        )\n",
        "    else:\n",
        "        dloader = get_test_loader(\n",
        "            config.data_dir, config.batch_size, **kwargs,\n",
        "        )\n",
        "\n",
        "    trainer = Trainer(config, dloader)\n",
        "\n",
        "    # either train\n",
        "    if config.is_train:\n",
        "        save_config(config)\n",
        "        trainer.train()\n",
        "    # or load a pretrained model and test\n",
        "    else:\n",
        "        trainer.test()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config, unparsed = get_config()\n",
        "    main(config)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}